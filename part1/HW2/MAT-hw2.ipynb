{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework 2\n",
    "\n",
    "### Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gd(x1, df, lr, n, time_limit=None):\n",
    "    t = time.time()\n",
    "    points = [x1]\n",
    "    for _ in range(n):\n",
    "        x = points[-1]\n",
    "        points.append(x - lr*df(x))\n",
    "        if time_limit is not None:\n",
    "            if time.time() - t > time_limit:\n",
    "                print('Time limit reached - gd')\n",
    "                break\n",
    "    return points\n",
    "\n",
    "def polyak_gd(x1, df, lr, mu, n, time_limit=None):\n",
    "    t = time.time()\n",
    "    points = [x1, x1]\n",
    "    for _ in range(n):\n",
    "        x = points[-1]\n",
    "        points.append(x - lr*df(x) + mu*(x - points[-2]))\n",
    "        if time_limit is not None:\n",
    "            if time.time() - t > time_limit:\n",
    "                print('Time limit reached - polyak')\n",
    "                break\n",
    "    return points\n",
    "\n",
    "def nesterov_gd(x1, df, lr, mu, n, time_limit=None):\n",
    "    t = time.time()\n",
    "    points = [x1, x1]\n",
    "    for _ in range(n):\n",
    "        x = points[-1]\n",
    "        points.append(x - lr*df(x + mu*(x - points[-2])) + mu*(x - points[-2]))\n",
    "        if time_limit is not None:\n",
    "            if time.time() - t > time_limit:\n",
    "                print('Time limit reached - nesterov')\n",
    "                break\n",
    "    return points\n",
    "\n",
    "def adagrad(x1, df, lr, n, time_limit=None):\n",
    "    t = time.time()\n",
    "    points = [x1]\n",
    "    d = np.ones([x1.shape[0], 1]) * 1e-10\n",
    "    for _ in range(n):\n",
    "        x = points[-1]\n",
    "        d += df(x)**2\n",
    "        D = np.diag(d**(-0.5))\n",
    "        points.append(x - lr*D*df(x))\n",
    "        if time_limit is not None:\n",
    "            if time.time() - t > time_limit:\n",
    "                print('Time limit reached - adagrad')\n",
    "                break\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0370359763344878e-09"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd(10, lambda x: 2*x, 0.1, 100)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.760651453586529e-11"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polyak_gd(10, lambda x: 2*x, 0.1, 0.1, 100)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1460793354650304e-10"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nesterov_gd(10, lambda x: 2*x, 0.1, 0.1, 100)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.88011605e-04],\n",
       "       [9.40058025e-05]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adagrad(np.array([[2], [1]]), lambda x: np.array([2*x[0], 2*x[1]]), 0.1, 1000)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def newton(x1, df, H, n, time_limit=None):\n",
    "    t = time.time()\n",
    "    points = [x1]\n",
    "    for _ in range(n):\n",
    "        p = points[-1]\n",
    "        points.append(p - np.linalg.inv(H(p)).dot(df(p)))\n",
    "        if time_limit is not None:\n",
    "            if time.time() - t > time_limit:\n",
    "                print('Time limit reached - newton')\n",
    "                break\n",
    "    return points\n",
    "\n",
    "\n",
    "def bfgs(x1, df, n, time_limit=None):\n",
    "    t = time.time()\n",
    "    B = [np.identity(x1.shape[0])]\n",
    "    points = [x1,\n",
    "              x1 - B[0].dot(df(x1))]\n",
    "    for _ in range(n-1):\n",
    "        p1 = points[-1]\n",
    "        p2 = points[-2]\n",
    "        gamma = df(p1) - df(p2)\n",
    "        delta = p1 - p2\n",
    "        if delta.T.dot(gamma) == 0:\n",
    "            print('bfgs: stopped because of invalid values')\n",
    "            return points\n",
    "        Bk = B[-1] - (delta.dot(gamma.T.dot(B[-1])) + B[-1].dot(gamma).dot(delta.T)) / (delta.T.dot(gamma)) + \\\n",
    "             (1 + (gamma.T.dot(B[-1]).dot(gamma)) / (delta.T.dot(gamma))) * (delta * delta.T) / (delta.T.dot(gamma))\n",
    "        \n",
    "        B.append(Bk)\n",
    "        points.append(p1 - Bk.dot(df(p1)))\n",
    "        if time_limit is not None:\n",
    "            if time.time() - t > time_limit:\n",
    "                print('Time limit reached - bfgs')\n",
    "                break\n",
    "    return points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function f1 has a global minimum at point at x\\* = (-1/6, -11/48, 1/6), f1(x\\*) = -19/96.\n",
    "\n",
    "Function f2 has a global minimum at point at x\\* = (1, 1, 1), f2(x\\*) = 0.\n",
    "\n",
    "Function f2 has a global minimum at point at x\\* = (3, 0.5), f3(x\\*) = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def df1(X):\n",
    "    x, y, z = X[0, 0], X[1, 0], X[2, 0]\n",
    "    return np.array([[34 * x - 16 * y + 6 * z + 1, -16 * x + 16 * y + 1, 6 * (x + z)]]).T\n",
    "\n",
    "def H1(X):\n",
    "    return np.array([[34, -16, 6], [-16, 16, 0], [6, 0, 6]])\n",
    "\n",
    "x1_11 = np.array([[0, 0, 0]]).T\n",
    "x1_12 = np.array([[1, 1, 0]]).T\n",
    "\n",
    "act_min1 = np.array([[-1/6, -11/48, 1/6]]).T\n",
    "\n",
    "def df2(X):\n",
    "    x, y, z = X[0, 0], X[1, 0], X[2, 0]\n",
    "    return np.array([[2*(x-1) - 400*x*(y-2*x**2), 2*(y-1) + 200*(y-x**2) - 400*y*(z-y**2), 200*(z-y**2)]]).T\n",
    "\n",
    "def H2(X):\n",
    "    x, y, z = X[0, 0], X[1, 0], X[2, 0]\n",
    "    return np.array([[-400*(y-x**2) + 800*x**2 + 2, -400*x, 0], [-400*x, -400*(z-y**2) + 800*y**2 + 202, -400*y],\n",
    "                     [0, -400*y, 200]])\n",
    "\n",
    "x1_21 = np.array([[1.2, 1.2, 1.2]]).T\n",
    "x1_22 = np.array([[-1, 1.2, 1.2]]).T\n",
    "\n",
    "act_min2 = np.array([[1, 1, 1]]).T\n",
    "\n",
    "def df3(X):\n",
    "    x, y = X[0, 0], X[1, 0]\n",
    "    return np.array([[2*(1.5 - x + x*y)*(y-1) + 2*(2.25 - x + x*y**2)*(y**2-1) + 2*(2.625 - x + x*y**3)*(y**3-1),\n",
    "                      2*(1.5 - x + x*y)*x + 4*(2.25 - x + x*y**2)*(x*y) + 6*(2.625 - x + x*y**3)*(x*y**2)]]).T\n",
    "\n",
    "def H3(X):\n",
    "    x, y = X[0, 0], X[1, 0]\n",
    "    return np.array([[2*(y**6+y**4-2*y**3-y**2-2*y+3), 2*x*(6*y**5+4*y**3 - 6*y**2 - 2*y - 2) + 15.75*y**2 + 9*y + 3],\n",
    "                     [2*x*(6*y**5 + 4*y**3 - 6*y**2 - 2*y - 2) + 15.75*y**2 + 9*y + 3,\n",
    "                      2*x*(x*(15*y**4 + 6*y**2 - 6*y - 1) + 6*2.625*y + 4.5*y)]])\n",
    "\n",
    "x1_31 = np.array([[1, 1]]).T\n",
    "x1_32 = np.array([[4.5, 4.5]]).T\n",
    "\n",
    "act_min3 = np.array([[3, 0.5]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compare_on_fn(grad, hess, start_points, actual_min, lr, mu):\n",
    "    pt1 = start_points[0]\n",
    "    pts1 = [gd(pt1, grad, lr, 100), polyak_gd(pt1, grad, lr, mu, 100),\n",
    "           nesterov_gd(pt1, grad, lr, mu, 100), adagrad(pt1, grad, lr, 100),\n",
    "           newton(pt1, grad, hess, 100), bfgs(pt1, grad, 100)]\n",
    "    pt2 = start_points[1]\n",
    "    pts2 = [gd(pt2, grad, lr, 100), polyak_gd(pt2, grad, lr, mu, 100),\n",
    "           nesterov_gd(pt2, grad, lr, mu, 100), adagrad(pt2, grad, lr, 100),\n",
    "           newton(pt2, grad, hess, 100), bfgs(pt2, grad, 100)]\n",
    "    names = ['GD', 'Polyak', 'Nesterov', 'AdaGrad', 'Newton', 'BFGS']\n",
    "    for j in range(6):\n",
    "        steps = np.array([2, 5, 10, 100])\n",
    "        st1 = steps.copy()\n",
    "        st2 = steps.copy()\n",
    "        if names[j] in ['Polyak', 'Nesterov']:  # two starting points saved\n",
    "            st1 += 1\n",
    "            st2 += 1\n",
    "        elif names[j] == 'BFGS':\n",
    "            st1 = [min(ss, len(pts1[j]) - 1) for ss in st1]\n",
    "            st2 = [min(ss, len(pts2[j]) - 1) for ss in st2]\n",
    "        print(f'Distance from actual minimum with method {names[j]}:'\n",
    "              f'- at starting point {pt1.T}: ')\n",
    "        for s in range(4):\n",
    "            dist = np.linalg.norm(pts1[j][st1[s]] - actual_min)\n",
    "            if names[j] == 'BFGS':\n",
    "                print(f'    - after {st1[s]} steps: {dist}')\n",
    "            else:\n",
    "                print(f'    - after {steps[s]} steps: {dist}')\n",
    "        print(f'- at starting point {pt2.T}: ')\n",
    "        for s in range(4):\n",
    "            dist = np.linalg.norm(pts2[j][st2[s]] - actual_min)\n",
    "            if names[j] == 'BFGS':\n",
    "                print(f'    - after {st1[s]} steps: {dist}')\n",
    "            else:\n",
    "                print(f'    - after {steps[s]} steps: {dist}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfgs: stopped because of invalid values\n",
      "bfgs: stopped because of invalid values\n",
      "Distance from actual minimum with method GD:- at starting point [[0 0 0]]: \n",
      "    - after 2 steps: 0.30556698665486315\n",
      "    - after 5 steps: 0.27542065817106137\n",
      "    - after 10 steps: 0.2339933570477582\n",
      "    - after 100 steps: 0.016388658000053968\n",
      "- at starting point [[1 1 0]]: \n",
      "    - after 2 steps: 1.4815978705438262\n",
      "    - after 5 steps: 1.258412758444076\n",
      "    - after 10 steps: 0.9987472242747498\n",
      "    - after 100 steps: 0.06377134179676562\n",
      "Distance from actual minimum with method Polyak:- at starting point [[0 0 0]]: \n",
      "    - after 2 steps: 0.29974130512827224\n",
      "    - after 5 steps: 0.24553780474157771\n",
      "    - after 10 steps: 0.1761915589972354\n",
      "    - after 100 steps: 0.0005586822297691529\n",
      "- at starting point [[1 1 0]]: \n",
      "    - after 2 steps: 1.4308940969431199\n",
      "    - after 5 steps: 1.052011154177836\n",
      "    - after 10 steps: 0.6941834468545387\n",
      "    - after 100 steps: 0.002173935573733558\n",
      "Distance from actual minimum with method Nesterov:- at starting point [[0 0 0]]: \n",
      "    - after 2 steps: 0.3000072499123979\n",
      "    - after 5 steps: 0.24719085908916322\n",
      "    - after 10 steps: 0.17912411715149462\n",
      "    - after 100 steps: 0.0006947737652309009\n",
      "- at starting point [[1 1 0]]: \n",
      "    - after 2 steps: 1.4356028060249348\n",
      "    - after 5 steps: 1.0685199936259653\n",
      "    - after 10 steps: 0.7126354585437319\n",
      "    - after 100 steps: 0.0027034928326898884\n",
      "Distance from actual minimum with method AdaGrad:- at starting point [[0 0 0]]: \n",
      "    - after 2 steps: 0.308099883983341\n",
      "    - after 5 steps: 0.28876905168952394\n",
      "    - after 10 steps: 0.26553103653190824\n",
      "    - after 100 steps: 0.1074696051318356\n",
      "- at starting point [[1 1 0]]: \n",
      "    - after 2 steps: 1.6910994152202843\n",
      "    - after 5 steps: 1.6806645444117148\n",
      "    - after 10 steps: 1.668471441052455\n",
      "    - after 100 steps: 1.5770011430158288\n",
      "Distance from actual minimum with method Newton:- at starting point [[0 0 0]]: \n",
      "    - after 2 steps: 4.8074067159589095e-17\n",
      "    - after 5 steps: 4.8074067159589095e-17\n",
      "    - after 10 steps: 4.8074067159589095e-17\n",
      "    - after 100 steps: 4.8074067159589095e-17\n",
      "- at starting point [[1 1 0]]: \n",
      "    - after 2 steps: 0.0\n",
      "    - after 5 steps: 4.8074067159589095e-17\n",
      "    - after 10 steps: 4.8074067159589095e-17\n",
      "    - after 100 steps: 4.8074067159589095e-17\n",
      "Distance from actual minimum with method BFGS:- at starting point [[0 0 0]]: \n",
      "    - after 2 steps: 2.1691692278125476\n",
      "    - after 5 steps: 5.731544418913148\n",
      "    - after 10 steps: 8.061634752119501e-07\n",
      "    - after 16 steps: 4.8074067159589095e-17\n",
      "- at starting point [[1 1 0]]: \n",
      "    - after 2 steps: 9.763697748171786\n",
      "    - after 5 steps: 5.802456925888386\n",
      "    - after 10 steps: 4.8074067159589095e-17\n",
      "    - after 16 steps: 4.8074067159589095e-17\n"
     ]
    }
   ],
   "source": [
    "compare_on_fn(df1, H1, [x1_11, x1_12], act_min1, 1e-2, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see, that Newton method works best already from the second step on. Later it doesn't improve anymore. After 16 steps, BFGS reaches the same distance, and after that, we stopped optimization, because of invalid values. We chose learning rate 0.01, because it is the best for first order methods (with higher one they diverge, with lower they are too slow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from actual minimum with method GD:- at starting point [[1.2 1.2 1.2]]: \n",
      "    - after 2 steps: 0.33723321009653945\n",
      "    - after 5 steps: 0.32515328779321634\n",
      "    - after 10 steps: 0.30885732488867595\n",
      "    - after 100 steps: 0.2824409186767944\n",
      "- at starting point [[-1.   1.2  1.2]]: \n",
      "    - after 2 steps: 2.0133294915561417\n",
      "    - after 5 steps: 2.003882775340714\n",
      "    - after 10 steps: 1.9891437488760535\n",
      "    - after 100 steps: 1.8404417947433258\n",
      "Distance from actual minimum with method Polyak:- at starting point [[1.2 1.2 1.2]]: \n",
      "    - after 2 steps: 0.3349918827710254\n",
      "    - after 5 steps: 0.31394151400798653\n",
      "    - after 10 steps: 0.2895622904586029\n",
      "    - after 100 steps: 0.31126716413940825\n",
      "- at starting point [[-1.   1.2  1.2]]: \n",
      "    - after 2 steps: 2.011673388931375\n",
      "    - after 5 steps: 1.9942232775800572\n",
      "    - after 10 steps: 1.9664592480814138\n",
      "    - after 100 steps: 1.7734360050343498\n",
      "Distance from actual minimum with method Nesterov:- at starting point [[1.2 1.2 1.2]]: \n",
      "    - after 2 steps: 0.3350453111403129\n",
      "    - after 5 steps: 0.3142769409765025\n",
      "    - after 10 steps: 0.2899964249637819\n",
      "    - after 100 steps: 0.31027674914969544\n",
      "- at starting point [[-1.   1.2  1.2]]: \n",
      "    - after 2 steps: 2.0117010098547987\n",
      "    - after 5 steps: 1.9944534879440732\n",
      "    - after 10 steps: 1.9670627685574773\n",
      "    - after 100 steps: 1.7742119173636788\n",
      "Distance from actual minimum with method AdaGrad:- at starting point [[1.2 1.2 1.2]]: \n",
      "    - after 2 steps: 0.34640006642290166\n",
      "    - after 5 steps: 0.3463910514548806\n",
      "    - after 10 steps: 0.3463804716185398\n",
      "    - after 100 steps: 0.34630026817841714\n",
      "- at starting point [[-1.   1.2  1.2]]: \n",
      "    - after 2 steps: 2.019883523652032\n",
      "    - after 5 steps: 2.0198679277303686\n",
      "    - after 10 steps: 2.0198496239660297\n",
      "    - after 100 steps: 2.019710844502459\n",
      "Distance from actual minimum with method Newton:- at starting point [[1.2 1.2 1.2]]: \n",
      "    - after 2 steps: 2.2703870285605476\n",
      "    - after 5 steps: 1.7218818175697506\n",
      "    - after 10 steps: 2.8826583848722285\n",
      "    - after 100 steps: 1.7480597483416782\n",
      "- at starting point [[-1.   1.2  1.2]]: \n",
      "    - after 2 steps: 3.1583713552748773\n",
      "    - after 5 steps: 1.7317001804209926\n",
      "    - after 10 steps: 2.281529499872278\n",
      "    - after 100 steps: 1.7437871938517366\n",
      "Distance from actual minimum with method BFGS:- at starting point [[1.2 1.2 1.2]]: \n",
      "    - after 2 steps: 83.34104882295058\n",
      "    - after 5 steps: 2.758244475350653e+18\n",
      "    - after 10 steps: nan\n",
      "    - after 100 steps: nan\n",
      "- at starting point [[-1.   1.2  1.2]]: \n",
      "    - after 2 steps: 173.66352238218667\n",
      "    - after 5 steps: 1.1330215810675727e+24\n",
      "    - after 10 steps: nan\n",
      "    - after 100 steps: nan\n"
     ]
    }
   ],
   "source": [
    "compare_on_fn(df2, H2, [x1_21, x1_22], act_min2, 1e-5, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here BFGS diverges, the reason could be big distance of the starting point from the actual minimum, at least for the second point. Newton method, previosly best method is jumping over minimum. The best ones are normal and Polyak gradient descends, depends on the step we are on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from actual minimum with method GD:- at starting point [[1 1]]: \n",
      "    - after 2 steps: 2.0615393524986976\n",
      "    - after 5 steps: 2.0615191625244966\n",
      "    - after 10 steps: 2.0614855138754815\n",
      "    - after 100 steps: 2.0608801149916327\n",
      "- at starting point [[4.5 4.5]]: \n",
      "    - after 2 steps: 3.8488791340005966\n",
      "    - after 5 steps: 3.487024466221991\n",
      "    - after 10 steps: 3.1350931351593223\n",
      "    - after 100 steps: 1.8941802320781476\n",
      "Distance from actual minimum with method Polyak:- at starting point [[1 1]]: \n",
      "    - after 2 steps: 2.061535987712399\n",
      "    - after 5 steps: 2.0614985553137104\n",
      "    - after 10 steps: 2.0614316757539686\n",
      "    - after 100 steps: 2.060221625120506\n",
      "- at starting point [[4.5 4.5]]: \n",
      "    - after 2 steps: 3.727437724879665\n",
      "    - after 5 steps: 3.064861130132364\n",
      "    - after 10 steps: 2.618243300682049\n",
      "    - after 100 steps: 1.554836009243065\n",
      "Distance from actual minimum with method Nesterov:- at starting point [[1 1]]: \n",
      "    - after 2 steps: 2.0615359875693717\n",
      "    - after 5 steps: 2.061498554026485\n",
      "    - after 10 steps: 2.0614316717426933\n",
      "    - after 100 steps: 2.0602215699365036\n",
      "- at starting point [[4.5 4.5]]: \n",
      "    - after 2 steps: 3.7532770524996395\n",
      "    - after 5 steps: 3.1564072553644924\n",
      "    - after 10 steps: 2.7059460146769436\n",
      "    - after 100 steps: 1.5705056921186153\n",
      "Distance from actual minimum with method AdaGrad:- at starting point [[1 1]]: \n",
      "    - after 2 steps: 3.029129029793891\n",
      "    - after 5 steps: 3.029128346192053\n",
      "    - after 10 steps: 3.029127610553488\n",
      "    - after 100 steps: 3.0291223300472563\n",
      "- at starting point [[4.5 4.5]]: \n",
      "    - after 2 steps: 4.27199649509584\n",
      "    - after 5 steps: 4.2719916925745745\n",
      "    - after 10 steps: 4.271986056034919\n",
      "    - after 100 steps: 4.271943314068065\n",
      "Distance from actual minimum with method Newton:- at starting point [[1 1]]: \n",
      "    - after 2 steps: 3.0413812651491097\n",
      "    - after 5 steps: 3.0413812651491097\n",
      "    - after 10 steps: 3.0413812651491097\n",
      "    - after 100 steps: 3.0413812651491097\n",
      "- at starting point [[4.5 4.5]]: \n",
      "    - after 2 steps: 2.8134963840566183\n",
      "    - after 5 steps: 1.8771366235122218\n",
      "    - after 10 steps: 2.9296264336411166\n",
      "    - after 100 steps: 3.0413812651491097\n",
      "Distance from actual minimum with method BFGS:- at starting point [[1 1]]: \n",
      "    - after 2 steps: 2222.160383709509\n",
      "    - after 5 steps: inf\n",
      "    - after 10 steps: nan\n",
      "    - after 100 steps: nan\n",
      "- at starting point [[4.5 4.5]]: \n",
      "    - after 2 steps: 7.466717015953301e+21\n",
      "    - after 5 steps: nan\n",
      "    - after 10 steps: nan\n",
      "    - after 100 steps: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ursau\\AppData\\Local\\Programs\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\ursau\\AppData\\Local\\Programs\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\ursau\\AppData\\Local\\Programs\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: overflow encountered in double_scalars\n",
      "C:\\Users\\ursau\\AppData\\Local\\Programs\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "compare_on_fn(df3, H3, [x1_31, x1_32], act_min3, 1e-6, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we needed to set learning rate as small, because otherwise most of our methods divirged (specially at the second starting point). BFGS again divirges. Newton jumpes over the minimum for the second point, on which the best one is Polyak. On the first point, the best are Polyak and Nesterov (slightly better), reaching almost the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_on_fn_time(grad, hess, start_points, actual_min, lr, mu):\n",
    "    pt1 = start_points[0]\n",
    "    pt2 = start_points[1]\n",
    "    names = ['GD', 'Polyak', 'Nesterov', 'AdaGrad', 'Newton', 'BFGS']\n",
    "    time_limits = [.1, 1, 2]\n",
    "    \n",
    "    points = [[[gd(pt, grad, lr, 1000000, t)[-1], polyak_gd(pt, grad, lr, mu, 1000000, t)[-1],\n",
    "             nesterov_gd(pt, grad, lr, mu, 1000000, t)[-1], adagrad(pt, grad, lr, 1000000, t)[-1],\n",
    "             newton(pt, grad, hess, 100000, t)[-1], bfgs(pt, grad, 100000, t)[-1]] for t in time_limits] for pt in start_points]\n",
    "    for j in range(6):\n",
    "        print(f'Distance from actual minimum with method {names[j]}:'\n",
    "              f'- at starting point {pt1.T}: ')\n",
    "        for t in range(3):\n",
    "            dist = np.linalg.norm(points[0][t][j] - actual_min)\n",
    "            print(f'    - after {time_limits[t]}s: {dist}')\n",
    "        print(f'- at starting point {pt2.T}: ')\n",
    "        for t in range(3):\n",
    "            dist = np.linalg.norm(points[1][t][j] - actual_min)\n",
    "            print(f'    - after {time_limits[t]}s: {dist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "bfgs: stopped because of invalid values\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "bfgs: stopped because of invalid values\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "bfgs: stopped because of invalid values\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "bfgs: stopped because of invalid values\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "bfgs: stopped because of invalid values\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "bfgs: stopped because of invalid values\n",
      "Distance from actual minimum with method GD:- at starting point [[0 0 0]]: \n",
      "    - after 0.1s: 7.696856831808123e-16\n",
      "    - after 1s: 7.696856831808123e-16\n",
      "    - after 2s: 7.696856831808123e-16\n",
      "- at starting point [[1 1 0]]: \n",
      "    - after 0.1s: 7.696856831808123e-16\n",
      "    - after 1s: 7.696856831808123e-16\n",
      "    - after 2s: 7.696856831808123e-16\n",
      "Distance from actual minimum with method Polyak:- at starting point [[0 0 0]]: \n",
      "    - after 0.1s: 6.150224720102942e-16\n",
      "    - after 1s: 6.150224720102942e-16\n",
      "    - after 2s: 6.150224720102942e-16\n",
      "- at starting point [[1 1 0]]: \n",
      "    - after 0.1s: 6.150224720102942e-16\n",
      "    - after 1s: 6.150224720102942e-16\n",
      "    - after 2s: 6.150224720102942e-16\n",
      "Distance from actual minimum with method Nesterov:- at starting point [[0 0 0]]: \n",
      "    - after 0.1s: 4.535298425247855e-16\n",
      "    - after 1s: 4.535298425247855e-16\n",
      "    - after 2s: 4.535298425247855e-16\n",
      "- at starting point [[1 1 0]]: \n",
      "    - after 0.1s: 4.535298425247855e-16\n",
      "    - after 1s: 4.535298425247855e-16\n",
      "    - after 2s: 4.535298425247855e-16\n",
      "Distance from actual minimum with method AdaGrad:- at starting point [[0 0 0]]: \n",
      "    - after 0.1s: 2.7065600190040662e-15\n",
      "    - after 1s: 2.7065600190040662e-15\n",
      "    - after 2s: 2.7065600190040662e-15\n",
      "- at starting point [[1 1 0]]: \n",
      "    - after 0.1s: 0.7701025830102706\n",
      "    - after 1s: 0.017811660675709145\n",
      "    - after 2s: 0.0003717178836771047\n",
      "Distance from actual minimum with method Newton:- at starting point [[0 0 0]]: \n",
      "    - after 0.1s: 4.8074067159589095e-17\n",
      "    - after 1s: 4.8074067159589095e-17\n",
      "    - after 2s: 4.8074067159589095e-17\n",
      "- at starting point [[1 1 0]]: \n",
      "    - after 0.1s: 4.8074067159589095e-17\n",
      "    - after 1s: 4.8074067159589095e-17\n",
      "    - after 2s: 4.8074067159589095e-17\n",
      "Distance from actual minimum with method BFGS:- at starting point [[0 0 0]]: \n",
      "    - after 0.1s: 4.8074067159589095e-17\n",
      "    - after 1s: 4.8074067159589095e-17\n",
      "    - after 2s: 4.8074067159589095e-17\n",
      "- at starting point [[1 1 0]]: \n",
      "    - after 0.1s: 4.8074067159589095e-17\n",
      "    - after 1s: 4.8074067159589095e-17\n",
      "    - after 2s: 4.8074067159589095e-17\n"
     ]
    }
   ],
   "source": [
    "compare_on_fn_time(df1, H1, [x1_11, x1_12], act_min1, 1e-2, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The best ones for the first function are also from timing point of view Newton and BFGS, very quickly reaching their optimum, better than all others already in 0.1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "Time limit reached - bfgs\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "Time limit reached - bfgs\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "Time limit reached - bfgs\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "Time limit reached - bfgs\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "Time limit reached - bfgs\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "Time limit reached - bfgs\n",
      "Distance from actual minimum with method GD:- at starting point [[1.2 1.2 1.2]]: \n",
      "    - after 0.1s: 1.5874583428801803\n",
      "    - after 1s: 1.608376111156007\n",
      "    - after 2s: 1.608376111156007\n",
      "- at starting point [[-1.   1.2  1.2]]: \n",
      "    - after 0.1s: 1.7799271628533668\n",
      "    - after 1s: 1.608376111156127\n",
      "    - after 2s: 1.6083761111560695\n",
      "Distance from actual minimum with method Polyak:- at starting point [[1.2 1.2 1.2]]: \n",
      "    - after 0.1s: 1.6071436201322986\n",
      "    - after 1s: 1.608376111156007\n",
      "    - after 2s: 1.608376111156007\n",
      "- at starting point [[-1.   1.2  1.2]]: \n",
      "    - after 0.1s: 1.6981278619328228\n",
      "    - after 1s: 1.6083761111560695\n",
      "    - after 2s: 1.6083761111560695\n",
      "Distance from actual minimum with method Nesterov:- at starting point [[1.2 1.2 1.2]]: \n",
      "    - after 0.1s: 1.604777723066266\n",
      "    - after 1s: 1.608376111156007\n",
      "    - after 2s: 1.608376111156007\n",
      "- at starting point [[-1.   1.2  1.2]]: \n",
      "    - after 0.1s: 1.7270794878184277\n",
      "    - after 1s: 1.6083761111560695\n",
      "    - after 2s: 1.6083761111560695\n",
      "Distance from actual minimum with method AdaGrad:- at starting point [[1.2 1.2 1.2]]: \n",
      "    - after 0.1s: 0.345697990259645\n",
      "    - after 1s: 0.34407948485148604\n",
      "    - after 2s: 0.34324534616338803\n",
      "- at starting point [[-1.   1.2  1.2]]: \n",
      "    - after 0.1s: 2.018653768925476\n",
      "    - after 1s: 2.0159710827226003\n",
      "    - after 2s: 2.014319876432343\n",
      "Distance from actual minimum with method Newton:- at starting point [[1.2 1.2 1.2]]: \n",
      "    - after 0.1s: 3.9739518017238806\n",
      "    - after 1s: 6.8866543012366295\n",
      "    - after 2s: 3.2556969890117182\n",
      "- at starting point [[-1.   1.2  1.2]]: \n",
      "    - after 0.1s: 1.7034124339171002\n",
      "    - after 1s: 1.7921070405092232\n",
      "    - after 2s: 2.9441226121924147\n",
      "Distance from actual minimum with method BFGS:- at starting point [[1.2 1.2 1.2]]: \n",
      "    - after 0.1s: nan\n",
      "    - after 1s: nan\n",
      "    - after 2s: nan\n",
      "- at starting point [[-1.   1.2  1.2]]: \n",
      "    - after 0.1s: nan\n",
      "    - after 1s: nan\n",
      "    - after 2s: nan\n"
     ]
    }
   ],
   "source": [
    "compare_on_fn_time(df2, H2, [x1_21, x1_22], act_min2, 1e-5, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the first starting point, the best results are reached by AdaGrad, again reaching better distance then all others in the first 0.1s. For the second point, gradient descends (normal, polyak and nesterov) work the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "Time limit reached - bfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ursau\\AppData\\Local\\Programs\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\ursau\\AppData\\Local\\Programs\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "Time limit reached - bfgs\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "Time limit reached - bfgs\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ursau\\AppData\\Local\\Programs\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: overflow encountered in double_scalars\n",
      "C:\\Users\\ursau\\AppData\\Local\\Programs\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time limit reached - bfgs\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "Time limit reached - bfgs\n",
      "Time limit reached - gd\n",
      "Time limit reached - polyak\n",
      "Time limit reached - nesterov\n",
      "Time limit reached - adagrad\n",
      "Time limit reached - newton\n",
      "Time limit reached - bfgs\n",
      "Distance from actual minimum with method GD:- at starting point [[1 1]]: \n",
      "    - after 0.1s: 2.0135193289455002\n",
      "    - after 1s: 1.6067950624636331\n",
      "    - after 2s: 1.341349813172955\n",
      "- at starting point [[4.5 4.5]]: \n",
      "    - after 0.1s: 0.8120581424505792\n",
      "    - after 1s: 0.7889390054797938\n",
      "    - after 2s: 0.7824087473186083\n",
      "Distance from actual minimum with method Polyak:- at starting point [[1 1]]: \n",
      "    - after 0.1s: 1.9722304427953623\n",
      "    - after 1s: 1.3849181379892868\n",
      "    - after 2s: 1.0837184376590137\n",
      "- at starting point [[4.5 4.5]]: \n",
      "    - after 0.1s: 0.7715587911361718\n",
      "    - after 1s: 0.7564268702572243\n",
      "    - after 2s: 0.7450198055840046\n",
      "Distance from actual minimum with method Nesterov:- at starting point [[1 1]]: \n",
      "    - after 0.1s: 1.9799060405613147\n",
      "    - after 1s: 1.4304651108319129\n",
      "    - after 2s: 1.1310410205108408\n",
      "- at starting point [[4.5 4.5]]: \n",
      "    - after 0.1s: 0.7853332778373325\n",
      "    - after 1s: 0.7690788369393639\n",
      "    - after 2s: 0.7587926906344681\n",
      "Distance from actual minimum with method AdaGrad:- at starting point [[1 1]]: \n",
      "    - after 0.1s: 3.0290883169304665\n",
      "    - after 1s: 3.0289965602371045\n",
      "    - after 2s: 3.028941853117684\n",
      "- at starting point [[4.5 4.5]]: \n",
      "    - after 0.1s: 4.27166023824025\n",
      "    - after 1s: 4.270903382265621\n",
      "    - after 2s: 4.270451845864263\n",
      "Distance from actual minimum with method Newton:- at starting point [[1 1]]: \n",
      "    - after 0.1s: 3.0413812651491097\n",
      "    - after 1s: 3.0413812651491097\n",
      "    - after 2s: 3.0413812651491097\n",
      "- at starting point [[4.5 4.5]]: \n",
      "    - after 0.1s: 3.0413812651491097\n",
      "    - after 1s: 3.0413812651491097\n",
      "    - after 2s: 3.0413812651491097\n",
      "Distance from actual minimum with method BFGS:- at starting point [[1 1]]: \n",
      "    - after 0.1s: nan\n",
      "    - after 1s: nan\n",
      "    - after 2s: nan\n",
      "- at starting point [[4.5 4.5]]: \n",
      "    - after 0.1s: nan\n",
      "    - after 1s: nan\n",
      "    - after 2s: nan\n"
     ]
    }
   ],
   "source": [
    "compare_on_fn_time(df3, H3, [x1_31, x1_32], act_min3, 1e-6, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For our last function, the method of choice is Polyak GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(N=100):\n",
    "    random.seed(0)\n",
    "\n",
    "    noise = np.array([random.random() for _ in range(N)])\n",
    "\n",
    "    X = np.array(range(1,N+1)) / N\n",
    "    Y = X + noise / N\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sgd_linear(pt1, lr, steps, X, Y):\n",
    "    points = [pt1]\n",
    "    for _ in range(steps):\n",
    "        pt = points[-1]\n",
    "        i = np.random.randint(0, len(X))\n",
    "        x, y = X[i], Y[i]\n",
    "        points.append(pt - lr*np.array([[x], [1]]) * 2*(pt[0]*x + pt[1] - y))\n",
    "    return points\n",
    "\n",
    "def l_bfgs(pt1, df, m, steps):\n",
    "    points = [pt1, pt1 - df(pt1)]\n",
    "    gammas = [df(points[0]) - df(points[1])]\n",
    "    deltas = [df(pt1)]                          # pt1 - points[1]\n",
    "    ros = [1/deltas[0].T.dot(gammas[0])]\n",
    "    for s in range(steps):\n",
    "\n",
    "        q = df(points[-1])\n",
    "        if len(gammas) > m:\n",
    "            gammas.pop(0)\n",
    "            deltas.pop(0)\n",
    "            ros.pop(0)\n",
    "        alpha = np.zeros(len(deltas))\n",
    "        for i in range(len(deltas)-1, -1, -1):\n",
    "            alpha[i] = ros[i] * deltas[i].T.dot(q)\n",
    "            q -= alpha[i] * gammas[i]\n",
    "        r = (deltas[-1].T.dot(gammas[-1]) / gammas[-1].T.dot(gammas[-1])) * q\n",
    "        for i in range(len(deltas)):\n",
    "            beta = ros[i] * gammas[i].T.dot(r)\n",
    "            r += deltas[i] * (alpha[i] - beta)\n",
    "\n",
    "        prev_pt = points[-1]\n",
    "        new_pt = prev_pt - r\n",
    "        gamma = df(new_pt) - df(prev_pt)\n",
    "        delta = new_pt - prev_pt\n",
    "        ro = 1/gamma.T.dot(delta)\n",
    "\n",
    "        points.append(new_pt)\n",
    "        gammas.append(gamma)\n",
    "        deltas.append(delta)\n",
    "        ros.append(ro)\n",
    "\n",
    "    return points\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N = 50\n",
    "X, Y = prepare_data(N)\n",
    "\n",
    "def f(k, n):\n",
    "    return sum((k*X + n - Y)**2) / N\n",
    "\n",
    "def gradf(par):\n",
    "    k, n = par[0, 0], par[1, 0]\n",
    "    return np.array([[2*(k*X + n - Y).dot(X), sum(2*(k*X + n - Y))]]).T / N\n",
    "\n",
    "def Hf(par):\n",
    "    return np.array([[2*sum(X**2)/ N, 2*sum(X)/ N], [2*sum(X)/ N, 2]])\n",
    "\n",
    "t = time.time()\n",
    "print(f'Gradient descent: {gd(np.array([[0],[1]]), gradf, 0.01, 5000)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'SGD: {sgd_linear(np.array([[0],[1]]), 0.01, 5000, X, Y)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'Newton: {newton(np.array([[0],[1]]), gradf, Hf, 200)[-1].T} (200 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'BFGS: {bfgs(np.array([[0],[1]]), gradf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'L-BFGS: {l_bfgs(np.array([[0],[1]]), gradf, 5, 20)[-1].T} (20 steps in {time.time()-t})')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N = 100\n",
    "X, Y = prepare_data(N)\n",
    "\n",
    "def f(k, n):\n",
    "    return sum((k*X + n - Y)**2) / N\n",
    "\n",
    "def gradf(par):\n",
    "    k, n = par[0, 0], par[1, 0]\n",
    "    return np.array([[2*(k*X + n - Y).dot(X), sum(2*(k*X + n - Y))]]).T / N\n",
    "\n",
    "def Hf(par):\n",
    "    return np.array([[2*sum(X**2)/ N, 2*sum(X)/ N], [2*sum(X)/ N, 2]])\n",
    "\n",
    "t = time.time()\n",
    "print(f'Gradient descent: {gd(np.array([[0],[1]]), gradf, 0.01, 5000)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'SGD: {sgd_linear(np.array([[0],[1]]), 0.01, 5000, X, Y)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'Newton: {newton(np.array([[0],[1]]), gradf, Hf, 200)[-1].T} (200 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'BFGS: {bfgs(np.array([[0],[1]]), gradf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'L-BFGS: {l_bfgs(np.array([[0],[1]]), gradf, 5, 20)[-1].T} (20 steps in {time.time()-t})')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N = 1000\n",
    "X, Y = prepare_data(N)\n",
    "\n",
    "def f(k, n):\n",
    "    return sum((k*X + n - Y)**2) / N\n",
    "\n",
    "def gradf(par):\n",
    "    k, n = par[0, 0], par[1, 0]\n",
    "    return np.array([[2*(k*X + n - Y).dot(X), sum(2*(k*X + n - Y))]]).T / N\n",
    "\n",
    "def Hf(par):\n",
    "    return np.array([[2*sum(X**2)/ N, 2*sum(X)/ N], [2*sum(X)/ N, 2]])\n",
    "\n",
    "t = time.time()\n",
    "print(f'Gradient descent: {gd(np.array([[0],[1]]), gradf, 0.01, 5000)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'SGD: {sgd_linear(np.array([[0],[1]]), 0.01, 5000, X, Y)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'Newton: {newton(np.array([[0],[1]]), gradf, Hf, 200)[-1].T} (200 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'BFGS: {bfgs(np.array([[0],[1]]), gradf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'L-BFGS: {l_bfgs(np.array([[0],[1]]), gradf, 5, 20)[-1].T} (20 steps in {time.time()-t})')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N = 10000\n",
    "X, Y = prepare_data(N)\n",
    "\n",
    "def f(k, n):\n",
    "    return sum((k*X + n - Y)**2) / N\n",
    "\n",
    "def gradf(par):\n",
    "    k, n = par[0, 0], par[1, 0]\n",
    "    return np.array([[2*(k*X + n - Y).dot(X), sum(2*(k*X + n - Y))]]).T / N\n",
    "\n",
    "def Hf(par):\n",
    "    return np.array([[2*sum(X**2)/ N, 2*sum(X)/ N], [2*sum(X)/ N, 2]])\n",
    "\n",
    "t = time.time()\n",
    "print(f'Gradient descent: {gd(np.array([[0],[1]]), gradf, 0.01, 1000)[-1].T} (1000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'SGD: {sgd_linear(np.array([[0],[1]]), 0.01, 5000, X, Y)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'Newton: {newton(np.array([[0],[1]]), gradf, Hf, 100)[-1].T} (100 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'BFGS: {bfgs(np.array([[0],[1]]), gradf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'L-BFGS: {l_bfgs(np.array([[0],[1]]), gradf, 5, 20)[-1].T} (20 steps in {time.time()-t})')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N = 100000\n",
    "X, Y = prepare_data(N)\n",
    "\n",
    "def f(k, n):\n",
    "    return sum((k*X + n - Y)**2) / N\n",
    "\n",
    "def gradf(par):\n",
    "    k, n = par[0, 0], par[1, 0]\n",
    "    return np.array([[2*(k*X + n - Y).dot(X), sum(2*(k*X + n - Y))]]).T / N\n",
    "\n",
    "def Hf(par):\n",
    "    return np.array([[2*sum(X**2)/ N, 2*sum(X)/ N], [2*sum(X)/ N, 2]])\n",
    "\n",
    "t = time.time()\n",
    "print(f'Gradient descent: {gd(np.array([[0],[1]]), gradf, 0.01, 500)[-1].T} (500 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'SGD: {sgd_linear(np.array([[0],[1]]), 0.01, 100000, X, Y)[-1].T} (100000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'Newton: {newton(np.array([[0],[1]]), gradf, Hf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'BFGS: {bfgs(np.array([[0],[1]]), gradf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'L-BFGS: {l_bfgs(np.array([[0],[1]]), gradf, 5, 20)[-1].T} (20 steps in {time.time()-t})')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "X, Y = prepare_data(N)\n",
    "\n",
    "def f(k, n):\n",
    "    return sum((k*X + n - Y)**2) / N\n",
    "\n",
    "def gradf(par):\n",
    "    k, n = par[0, 0], par[1, 0]\n",
    "    return np.array([[2*(k*X + n - Y).dot(X), sum(2*(k*X + n - Y))]]).T / N\n",
    "\n",
    "def Hf(par):\n",
    "    return np.array([[2*sum(X**2)/ N, 2*sum(X)/ N], [2*sum(X)/ N, 2]])\n",
    "\n",
    "t = time.time()\n",
    "print(f'Gradient descent: {gd(np.array([[0],[1]]), gradf, 0.01, 100)[-1].T} (100 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'SGD: {sgd_linear(np.array([[0],[1]]), 0.01, 1000000, X, Y)[-1].T} (1000000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'Newton: {newton(np.array([[0],[1]]), gradf, Hf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'BFGS: {bfgs(np.array([[0],[1]]), gradf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'L-BFGS: {l_bfgs(np.array([[0],[1]]), gradf, 5, 20)[-1].T} (20 steps in {time.time()-t})')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that second order methods are much better than GD, since they are faster and more precise, in less steps. GD fails in large datasets, since we would need too much time to perform enough steps for good results. But SGD has much faster steps, so it reaches similar performance as second order methods in the same time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "We can see that second order methods are much better than GD, since they are faster and more precise, in less steps. GD fails in large datasets, since we would need too much time to perform enough steps for good results. But SGD has much faster steps, so it reaches similar performance as second order methods in the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent: [[0.99652904 0.01332757]] (5000 steps in 0.12879157066345215)\n",
      "SGD: [[0.9968744  0.01258143]] (5000 steps in 0.06503891944885254)\n",
      "Newton: [[0.99827278 0.01237627]] (200 steps in 0.013986349105834961)\n",
      "bfgs: stopped because of invalid values\n",
      "BFGS: [[0.99827278 0.01237627]] (20 steps in 0.0020110607147216797)\n",
      "L-BFGS: [[0.99827278 0.01237627]] (20 steps in 0.0029997825622558594)\n"
     ]
    }
   ],
   "source": [
    "N = 50\n",
    "X, Y = prepare_data(N)\n",
    "\n",
    "def f(k, n):\n",
    "    return sum((k*X + n - Y)**2) / N\n",
    "\n",
    "def gradf(par):\n",
    "    k, n = par[0, 0], par[1, 0]\n",
    "    return np.array([[2*(k*X + n - Y).dot(X), sum(2*(k*X + n - Y))]]).T / N\n",
    "\n",
    "def Hf(par):\n",
    "    return np.array([[2*sum(X**2)/ N, 2*sum(X)/ N], [2*sum(X)/ N, 2]])\n",
    "\n",
    "t = time.time()\n",
    "print(f'Gradient descent: {gd(np.array([[0],[1]]), gradf, 0.01, 5000)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'SGD: {sgd_linear(np.array([[0],[1]]), 0.01, 5000, X, Y)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'Newton: {newton(np.array([[0],[1]]), gradf, Hf, 200)[-1].T} (200 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'BFGS: {bfgs(np.array([[0],[1]]), gradf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'L-BFGS: {l_bfgs(np.array([[0],[1]]), gradf, 5, 20)[-1].T} (20 steps in {time.time()-t})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent: [[0.99890635 0.00646922]] (5000 steps in 0.1719987392425537)\n",
      "SGD: [[0.99837604 0.0067175 ]] (5000 steps in 0.06699919700622559)\n",
      "Newton: [[1.00060802 0.00554968]] (200 steps in 0.019998788833618164)\n",
      "BFGS: [[1.00060802 0.00554968]] (20 steps in 0.003000497817993164)\n",
      "L-BFGS: [[1.00060802 0.00554968]] (20 steps in 0.00400090217590332)\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "X, Y = prepare_data(N)\n",
    "\n",
    "def f(k, n):\n",
    "    return sum((k*X + n - Y)**2) / N\n",
    "\n",
    "def gradf(par):\n",
    "    k, n = par[0, 0], par[1, 0]\n",
    "    return np.array([[2*(k*X + n - Y).dot(X), sum(2*(k*X + n - Y))]]).T / N\n",
    "\n",
    "def Hf(par):\n",
    "    return np.array([[2*sum(X**2)/ N, 2*sum(X)/ N], [2*sum(X)/ N, 2]])\n",
    "\n",
    "t = time.time()\n",
    "print(f'Gradient descent: {gd(np.array([[0],[1]]), gradf, 0.01, 5000)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'SGD: {sgd_linear(np.array([[0],[1]]), 0.01, 5000, X, Y)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'Newton: {newton(np.array([[0],[1]]), gradf, Hf, 200)[-1].T} (200 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'BFGS: {bfgs(np.array([[0],[1]]), gradf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'L-BFGS: {l_bfgs(np.array([[0],[1]]), gradf, 5, 20)[-1].T} (20 steps in {time.time()-t})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent: [[0.99829198 0.00140977]] (5000 steps in 0.8229990005493164)\n",
      "SGD: [[0.99846352 0.00127669]] (5000 steps in 0.0691986083984375)\n",
      "Newton: [[9.99954789e-01 5.18998970e-04]] (200 steps in 0.12718534469604492)\n",
      "BFGS: [[9.99954789e-01 5.18998970e-04]] (20 steps in 0.011997222900390625)\n",
      "L-BFGS: [[9.99954789e-01 5.18998970e-04]] (20 steps in 0.01399993896484375)\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "X, Y = prepare_data(N)\n",
    "\n",
    "def f(k, n):\n",
    "    return sum((k*X + n - Y)**2) / N\n",
    "\n",
    "def gradf(par):\n",
    "    k, n = par[0, 0], par[1, 0]\n",
    "    return np.array([[2*(k*X + n - Y).dot(X), sum(2*(k*X + n - Y))]]).T / N\n",
    "\n",
    "def Hf(par):\n",
    "    return np.array([[2*sum(X**2)/ N, 2*sum(X)/ N], [2*sum(X)/ N, 2]])\n",
    "\n",
    "t = time.time()\n",
    "print(f'Gradient descent: {gd(np.array([[0],[1]]), gradf, 0.01, 5000)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'SGD: {sgd_linear(np.array([[0],[1]]), 0.01, 5000, X, Y)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'Newton: {newton(np.array([[0],[1]]), gradf, Hf, 200)[-1].T} (200 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'BFGS: {bfgs(np.array([[0],[1]]), gradf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'L-BFGS: {l_bfgs(np.array([[0],[1]]), gradf, 5, 20)[-1].T} (20 steps in {time.time()-t})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent: [[0.67982156 0.17142094]] (1000 steps in 2.111002206802368)\n",
      "SGD: [[9.98438178e-01 9.06125523e-04]] (5000 steps in 0.08395934104919434)\n",
      "Newton: [[9.99998330e-01 5.09011207e-05]] (100 steps in 0.7059998512268066)\n",
      "BFGS: [[9.99998330e-01 5.09011207e-05]] (20 steps in 0.1100010871887207)\n",
      "L-BFGS: [[9.99998330e-01 5.09011207e-05]] (20 steps in 0.10381293296813965)\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "X, Y = prepare_data(N)\n",
    "\n",
    "def f(k, n):\n",
    "    return sum((k*X + n - Y)**2) / N\n",
    "\n",
    "def gradf(par):\n",
    "    k, n = par[0, 0], par[1, 0]\n",
    "    return np.array([[2*(k*X + n - Y).dot(X), sum(2*(k*X + n - Y))]]).T / N\n",
    "\n",
    "def Hf(par):\n",
    "    return np.array([[2*sum(X**2)/ N, 2*sum(X)/ N], [2*sum(X)/ N, 2]])\n",
    "\n",
    "t = time.time()\n",
    "print(f'Gradient descent: {gd(np.array([[0],[1]]), gradf, 0.01, 1000)[-1].T} (1000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'SGD: {sgd_linear(np.array([[0],[1]]), 0.01, 5000, X, Y)[-1].T} (5000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'Newton: {newton(np.array([[0],[1]]), gradf, Hf, 100)[-1].T} (100 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'BFGS: {bfgs(np.array([[0],[1]]), gradf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'L-BFGS: {l_bfgs(np.array([[0],[1]]), gradf, 5, 20)[-1].T} (20 steps in {time.time()-t})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent: [[0.38187357 0.33082066]] (500 steps in 9.945998668670654)\n",
      "SGD: [[9.99999657e-01 5.26353505e-06]] (100000 steps in 1.4070022106170654)\n",
      "Newton: [[1.00000002e+00 4.98516617e-06]] (20 steps in 1.568998098373413)\n",
      "BFGS: [[1.00000002e+00 4.98516617e-06]] (20 steps in 1.1319999694824219)\n",
      "L-BFGS: [[1.00000002e+00 4.98516617e-06]] (20 steps in 1.296036958694458)\n"
     ]
    }
   ],
   "source": [
    "N = 100000\n",
    "X, Y = prepare_data(N)\n",
    "\n",
    "def f(k, n):\n",
    "    return sum((k*X + n - Y)**2) / N\n",
    "\n",
    "def gradf(par):\n",
    "    k, n = par[0, 0], par[1, 0]\n",
    "    return np.array([[2*(k*X + n - Y).dot(X), sum(2*(k*X + n - Y))]]).T / N\n",
    "\n",
    "def Hf(par):\n",
    "    return np.array([[2*sum(X**2)/ N, 2*sum(X)/ N], [2*sum(X)/ N, 2]])\n",
    "\n",
    "t = time.time()\n",
    "print(f'Gradient descent: {gd(np.array([[0],[1]]), gradf, 0.01, 500)[-1].T} (500 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'SGD: {sgd_linear(np.array([[0],[1]]), 0.01, 100000, X, Y)[-1].T} (100000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'Newton: {newton(np.array([[0],[1]]), gradf, Hf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'BFGS: {bfgs(np.array([[0],[1]]), gradf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'L-BFGS: {l_bfgs(np.array([[0],[1]]), gradf, 5, 20)[-1].T} (20 steps in {time.time()-t})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent: [[-0.03142227  0.58765207]] (100 steps in 24.45909595489502)\n",
      "SGD: [[1.00000007e+00 5.46574430e-07]] (1000000 steps in 14.115238189697266)\n",
      "Newton: [[9.99999999e-01 5.00281263e-07]] (20 steps in 13.457749605178833)\n",
      "BFGS: [[9.99999999e-01 5.00281263e-07]] (20 steps in 13.58800745010376)\n",
      "L-BFGS: [[9.99999999e-01 5.00281263e-07]] (20 steps in 15.257956504821777)\n"
     ]
    }
   ],
   "source": [
    "N = 1000000\n",
    "X, Y = prepare_data(N)\n",
    "\n",
    "def f(k, n):\n",
    "    return sum((k*X + n - Y)**2) / N\n",
    "\n",
    "def gradf(par):\n",
    "    k, n = par[0, 0], par[1, 0]\n",
    "    return np.array([[2*(k*X + n - Y).dot(X), sum(2*(k*X + n - Y))]]).T / N\n",
    "\n",
    "def Hf(par):\n",
    "    return np.array([[2*sum(X**2)/ N, 2*sum(X)/ N], [2*sum(X)/ N, 2]])\n",
    "\n",
    "t = time.time()\n",
    "print(f'Gradient descent: {gd(np.array([[0],[1]]), gradf, 0.01, 100)[-1].T} (100 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'SGD: {sgd_linear(np.array([[0],[1]]), 0.01, 1000000, X, Y)[-1].T} (1000000 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'Newton: {newton(np.array([[0],[1]]), gradf, Hf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'BFGS: {bfgs(np.array([[0],[1]]), gradf, 20)[-1].T} (20 steps in {time.time()-t})')\n",
    "t = time.time()\n",
    "print(f'L-BFGS: {l_bfgs(np.array([[0],[1]]), gradf, 5, 20)[-1].T} (20 steps in {time.time()-t})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that second order methods are much better than GD, since they are faster and more precise, in less steps. GD fails in large datasets, since we would need too much time to perform enough steps for good results. But SGD has much faster steps, so it reaches similar performance as second order methods in the same time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}